package com.atguigu.api.sink;

import com.alibaba.fastjson.JSON;
import com.atguigu.bean.Event;
import com.atguigu.function.ClickSource;
import org.apache.flink.connector.base.DeliveryGuarantee;
import org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchema;
import org.apache.flink.connector.kafka.sink.KafkaSink;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.kafka.clients.producer.ProducerRecord;

import java.nio.charset.StandardCharsets;

public class Flink03_SinkKafkaWithKey {
    public static void main(String[] args) throws Exception {
        //åˆ›å»ºæµå¼å¤„ç†
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        //è®¾ç½®å…¨å±€å¹¶è¡Œåº¦ï¼šä¸è®¾ç½®é»˜è®¤ä¸ºå…¨å¹¶è¡Œåº¦ï¼›1ä¸ºå•çº¿ç¨‹æ‰§è¡Œ
        env.setParallelism(1);

        //å¼€å¯æ£€æŸ¥ç‚¹
        env.enableCheckpointing(5000);

        KafkaSink<Event> build = KafkaSink
                .<Event>builder()
                .setBootstrapServers("hadoop102:9092,hadoop103:9092,hadoop104:9092")
                .setRecordSerializer(
                        //å†™å…¥å¸¦Keyçš„æ¶ˆæ¯,éœ€è¦è‡ªå·±å®ç°åºåˆ—åŒ–è¿‡ç¨‹
                        (KafkaRecordSerializationSchema<Event>) (event, kafkaSinkContext, aLong) -> {
                            //æŒ‰ç…§keyåˆ†ç»„
                            byte[] key = event.getUser().getBytes(StandardCharsets.UTF_8);
                            byte[] value = JSON.toJSONString(event).getBytes(StandardCharsets.UTF_8);

                            return new ProducerRecord<>("topic_db", key, value);
                        }
                )
                .setDeliveryGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
                //å¦‚æœğŸ‘†æ˜¯ç²¾ç¡®ä¸€æ¬¡ï¼Œåˆ™å¿…é¡»è®¾ç½®äº‹åŠ¡IDï¼šğŸ‘‡
                .setTransactionalIdPrefix("flink")
                .build();

        env
                .addSource(new ClickSource())
                .sinkTo(build);

        //å¯åŠ¨ç¨‹åºæ‰§è¡Œ
        env.execute();
    }
}
